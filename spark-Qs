
Q 1: what would be following code do:
df.na.drop(2)

A. drop initial 2 rows
B. drop rows with 2 null values for columns
C. drop last 2 rows
d. drop rows with minimum 2 column values equal to null
E. drop rows with maximum 2 column values equal to null

Ans.: d. drop rows with minimum 2 column values equal to null

Explanation: specification of drop function with na is to drop rows considerimng minimum int value

Topic: Common 
Q 2: write operation on RDD is ?
A. coarse grained	
B. fine grained 
C. Both fine grained and coarse grained
D. Runs always on NODE_LOCAL
E. Run always on PROCESS_LOCAL

Ans.: A. Coarse grained

Explanation:  it means that you can write you transformations to be applied to the whole dataset, but not individual elements on the dataset. Operations like map, filter, group reduce, but not get(index) or set(index).

Q 3:  In Spark, RDD storage level defined as MEMORY_ONLY_2 , what does _2 means ?
A. replicas
B. partitions
C. shuffling level
D. buckets
E. executors

Ans.: A. Replicas

Explanation: storage level MEMORY_ONLY_2 indicates spark transformations are being carried in memory and there are 2 replicas maintained for the same




Q 4:  To adjust how long Spark will wait before it times out on each of the phases of data, spark.locality.* config can contain which of the following phase is not present in configuration?
A. data local
B. process local
C. node local
D. stage local

Ans.: D. Stage local

Explanation: 4 stages of configuration are data, process, node and rack local.




Topic: Advanced implementation
Q 5:  For 100 rules, spark.default.parallelism = 50, number of stages per task = 3, how much would be total number of tasks?
A. 15,000
B. 150
C. 5,000
D. 300
E. 1500

Ans.: A. 15,000
Explanation: number of jobs = 100
• number of stages per job = 3
• number of tasks per job = 3 * 50 = 150
• total number of tasks = 100 * 150 = 15,000




Q 6:  what would be output of following operations on spark dataframe:
val test = sqlContext.read.json(sc.parallelize(Seq("""{"a":1,"b":[2,3]}""")))
test.withColumn("b", explode($"b")).show

A. 
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  1|  3|
+---+---+
B. 
+---+---+
|  a|  b|
+---+---+
|  1|  2|
+---+---+
C.
+---+---+
|  a|  b|
+---+---+
|  1|  3|
+---+---+
D.
+---+
|  b|
+---+
|  2|
|  3|
+---+
E.
+---+
|  b|
+---+
|  2|
|  2|
+---+
Ans.: A. 
+---+---+
|  a|  b|
+---+---+
|  1|  2|
|  1|  3|
+---+---+

Explanation: 
Explode function on dataframe column is used to flatten nested columns or array of columns.

Q 7:  If your RDD is so large that all of it's elements won't fit in memory on the drive machine then which of the following is not recommended and would cause performance and memory issues?

A. take()
B. takeSample()
C. collect()
D. show()
E. top()

Ans.: C. collect()

Explanation: 
if RDD size is too large, then collect() is not recommended as it would take entire RDD into memory and data might be too big to fit into memory, you can write out RDD to files or export the RDD to a databse that is large enough to hold all the data.

Q 8:  which of the following is incorrect on spark joins?
A. if spark.sql.crossJoin.enabled = false then select * from A inner join B doesn’t throw an error
B. if spark.sql.crossJoin.enabled = false then select * from A cross join B doesn't throw an error
C. if spark.sql.crossJoin.enabled = true then select * from A inner join B doesn't throw an error
D.  if spark.sql.crossJoin.enabled = true then select * from A cross join B doesn't throw an error
Ans.: A. if spark.sql.crossJoin.enabled = false then select * from A inner join B doesn’t throw an error

Explanation: 
if spark.sql.crossjoin.enabled is set to false then inner join throws an error
Q 9.:  In spark to predict CPU/task resources, which of the following formulas or equations would be incorrect or not fit for calculation estimation of resources?
A. number of jobs = number of output ops
B. number of stages per job = number of shuffles + 1
C. number of tasks per job = number of stages * number of partitions
D. number of stages = number of tasks
E. number of tasks = number of jobs

Ans.: E. number of tasks = number of jobs
Explanation: 
Each job can have multiple stages and each stage can have number of tasks associated with it so number of tasks would be greater than number of jobs.

Q 10:  For an 8 core executor, with 15 second micro-batch duration and For 100 rules, spark.default.parallelism = 50, number of stages per task = 3, what would be average task time?
A. 324 ms
B. 394 ms
C. 424 ms
D. 494 ms
E. 294 ms
Ans.: B. 394 ms
Explanation: 
 number of jobs = 100
• number of stages per job = 3
• number of tasks per job = 3 * 50 = 150
• total number of tasks = 100 * 150 = 15,000
50 instances * 8 cores = 400 task slots
Number of waves = 15,000/400 = 38 waves
Q 11:  which of the following cannot be used for offset management in Spark version 1.6.0 with messaging queue kafka?
A. checkpoint(“checkpoint_dir”)
B. zookeeper
C. hbase
D. kafka
E. RDBMS

Ans.: D. Kafka

Explanation: Spark 2.1.0 onwards we can do offset management using kafka itself. In spark 1.6.0 or previous only zookeeper, hbase or any db, checkpoint where the only options available

Q 12: which of following is true for wide transformation?
A. Data required to compute resides on multiple partitions
B. Data required to compute resides on single partition
C. There can only be single replica for the dataset with wide transformation
d. No caching can be done for wide transformations
E. They are not lazily evaluated

Ans.: A. Data required to compute resides on multiple partitions

Explanation: Wide transformation means data resides on multiple partitions .
e.g. reduceByKey




Q 13: which of following statement is correct for following code:
df.filter($"High">500).count()*1.0/df.count())*100

A. Extract What percentage of the time was the High greater than $500
B. Extract What was mean of the time was the High greater than $500
C. Extract What was the count of the time was the High greater than $500
d. Extract What std dev of the time was the High greater than $500
E. Throws Error as can’t do multiplication and division on output of transformation

Ans.: A. Extract What percentage of the time was the High greater than $500

Explanation: This formula is used to calculate percentage in spark df


Q 14:  To maintain state while performing map operation i.e. in spark mapWithState which of the following is optional to be initialised?

A. checkpoint(“checkpoint_dir”)
B. partitions()
C. initialState(initialStateDataFrame)
D. StateFunc 
E. remember(60 seconds)

Ans.: E. Remember(60 seconds)

Explanation: Remember() function is optional and its not mandate to be used with mapWithState


Q 15:  When spark action is triggered, which of following configuration decides maximum number of output files in spark?
A. mapred.reduce.tasks
B. spark.sql.shuffle.partitions
C. spark.shuffle.tasks
d. spark.partitions
E. spark.coalesce.partitions

Ans.: B. spark.sql.shuffle.partitions

Explanation: spark.sql.shuffle.partitions is default set to 200, which means based on executor and number of cores, maximum 200 small files would get created


Q 16:  spark model evaluation with MulticlassMetrics, which of the following statement is incorrect?
A. Need to import org.apache.spark.mllib.evaluation.MulticlassMetrics
B. pNeed to instantiate with label as below
// Need to convert to RDD to use this
val predictionAndLabels = results.select($"prediction",$"label").as[(Double, Double)].rdd
// Instantiate metrics object
val metrics = new MulticlassMetrics(predictionAndLabels)

C. can create confusion matrix as metrics.confusionMatrix
D. can create two dimentional metrics as metrics.ofDim

Ans.: D. can create two dimentional metrics as metrics.ofDim

Explanation: There is no function called as ofDim on MulticlassMetrics
Q 17: which of following is not an transformation
A. reduceByKey
B. aggregateByKey
C. sumByKey
D. countByKey
E. sample

Ans.: c. SumByKey

Explanation: sumByKey is not an spark transformation

Q 18:  which of the following is incorrect way to calculate standard deviation using spark transformations?

A. selectedData.groupBy($"user").agg(stdev($"duration"))
B. selectedData.groupBy($"user").agg((sqrt( avg($"duration" * $"duration") - avg($"duration") * avg($"duration"))).alias("duration_sd"))
C. val stdevduration = duration.groupByKey().mapValues(value => org.apache.spark.util.StatCounter(value).stddev)
D. df.registerTempTable("df")
sqlContext.sql("""SELECT user, stddev(duration) FROM df GROUP BY user""")
E. df.registerTempTable("df")
hiveContext.sql("""SELECT user, stddev(duration) FROM df GROUP BY user""")

Ans.: A. selectedData.groupBy($"user").agg(stdev($"duration"))

Explanation: 
It would give compiler error on stddev as stddev is not valid function, right function is stddev_pop. Hence answer is: selectedData.groupBy($"user").agg(stdev_pop($"duration"))

Q 19:  which of the following does not take collection aggregateByKey or reduceByKey and throw error from following?
A. val resReduce = pairs.reduceByKey(_ + _) 
B. val resAgg = pairs.aggregateByKey(0)(_+_,_+_)
C. val sets = pairs.aggregateByKey(new HashSet[Int])(_+_, _++_)
D. val sets = pairs.reduceByKey(new HashSet[Int])(_ + _)
E. val sets = pairs.aggregateByKey(0)((k,v) => v.toInt +k,(v,k) => k+v)

Ans.: D. val sets = pairs.reduceByKey(new HashSet[Int])(_ + _)

Explanation: 
reduceByKey doesnt works on hashset and it works only between partitions

Q 20:  which of the following would not be helpful to avoid exception from following code:

JavaStreamingContext jssc = new JavaStreamingContext(sc, INTERVAL);

// This enables checkpointing.
jssc.checkpoint("/tmp/checkpoint_test");

JavaDStream<String> dStream = jssc.socketTextStream("localhost", 9999);

NotSerializable notSerializable = new NotSerializable();
dStream.foreachRDD(rdd -> {
      if (rdd.count() == 0) {
        return null;
      }
      String first = rdd.first();

      notSerializable.doSomething(first);
      return null;
    }
);

A. Turning off checkpointing

B. Make the object being used Serializable.

C. Declare NotSerializable inside forEachRDD function.

D. Removing notSerializable object creation code 

E. Use collect instead of foreachRDD

Ans.: D. Removing notSerializable object creation code 

Explanation: 
code would cause "ERROR OneForOneStrategy: java.io.NotSerializableException
To avoid exception any of option A, B or C, E should be done






38. Consider a dataframe df is ordered by Day in an increasing order, then by value in a descending order : 

+----+--------+----------+
|Day |Type    |Value 	 |
+----+--------+----------+
|   0|   cat26|      30.9|
|   0|   cat13|      22.1|
|   0|   cat95|      19.6|
|   0|  cat105|       1.3|
|   1|   cat67|      28.5|
|   1|    cat4|      26.8|
|   1|   cat13|      12.6|
|   1|   cat23|       5.3|
|   2|   cat56|      39.6|
|   2|   cat40|      29.7|
|   2|  cat187|      27.9|
|   2|   cat68|       9.8|
|   3|    cat8|      35.6|
| ...|    ....|      ....|
+----+--------+----------+

If we want expected result to be as
+----+--------+----------+
|Day |Type    |Value     |
+----+--------+----------+
|   0|   cat26|      30.9|
|   1|   cat67|      28.5|
|   2|   cat56|      39.6|
|   3|    cat8|      35.6|
| ...|     ...|       ...|
+----+--------+----------+

which of the following would be true:

A. val w = Window.partitionBy($"Day").orderBy($"Value".desc)
val dfTop = df.withColumn("rn", row_number.over(w)).where($"rn" === 1).drop("rn")
dfTop.show

B. val dfMax = df.groupBy($"Day".as("max_day")).agg(max($"Value").as("max_value"))
val dfTopByJoin = df.join(broadcast(dfMax),
    ($"Day" === $"max_day") && ($"Value" === $"max_value"))
  .drop("max_day")
  .drop("max_value")
  .show
  
C. val data = df.groupBy("Day").agg(first("Day").as("_1"),first("Type").as("Type"),first("Value").as("Value")).drop("Day")
data.withColumnRenamed("_1","Day").show

D. val keys = List("Day", "Type");
 val selectFirstValueOfNoneGroupedColumns = 
 df.columns
   .filterNot(keys.toSet)
   .map(_ -> "first").toMap
 val grouped = 
 df.groupBy(keys.head, keys.tail: _*)
   .agg(selectFirstValueOfNoneGroupedColumns)

E. sqlContext.sql("select Day, vs.Type, vs.Value from (select Day, max(struct(Value, Type)) as vs from table group by Day)").show(false)

Ans. A,B,E

Explanation: C, D won't produce expected result as they would produce as per open jira on Spark.



39. For following dataframe df:

+----+-----+---+-----+
| id |  key| va|cnt  |
+----+-----+---+-----+
|   a|pref1|  b|  168|
|   a|pref3|  h|  168|
|   a|pref3|  t|   63|
|   a|pref3|  k|   84|
|   a|pref1|  e|   84|
|   a|pref2|  z|  105|
+----+-----+---+-----+

how can we get
+----+-----+---+----------+
|  id|  key| va|max(cnt)  |
+----+-----+---+----------+
|   a|pref1|  b|       168|
|   a|pref3|  h|       168|
|   a|pref2|  z|       105|
+----+-----+---+----------+

Select from following which can lead to expected result:

A. val maxXount = df.groupBy("id", "key").agg(max("cnt") as "max(cnt)")
maxCount.join(df, Seq("id", "key")).where($"max(cnt)" === $"cnt").show

B. df.groupBy("id", "key").max("cnt").alias("max(cnt)")

C. w = Window.partitionBy("id", "key")
df.withColumn("max_cnt", max_("cnt").over(w))

D. val w = Window.partitionBy("id","key").orderBy(col("cnt").desc)
df.withColumn("rank", dense_rank().over(w)).select("id", "key","va","cnt").where("rank == 1").show

Ans. A,C,D

Explanation: B misses va column


40. Consider following dataframe df:

e_name  RPA_1    RPA_2    RPA_3    RPA_4
A  		  2        1       2.1      5.4
B	     1.5      0.5      0.9      3.7
C		 7        2.9      9.1      2.5

We need to extract max RPA for each employee. Expected result dataframe is:

|e_name  | max_RPA    |
|--------|------------|
| A		 | 5.4		  |
| B		 | 3.7	      |
| C		 | 9.1		  |

Choose the correct from following:

A. df.withColumn("max_RPA", maxval($"RPA_1", $"RPA_2", $"RPA_3", $"RPA_4"))
  .select("e_name", "max_RPA").show
  
B. val finalDf = df.withColumn("e_name", maxValAsMap(keys, values)).select("e_name", "max_RPA")

val maxValAsMap = udf((keys: Seq[String], values: Seq[Any]) => {
    val valueMap:Map[String,Double] = (keys zip values).filter( _._2.isInstanceOf[Double] ).map{
      case (x,y) => (x, y.asInstanceOf[Double])
    }.toMap
    if (valueMap.isEmpty) "not computed" else valueMap.maxBy(_._2)._1
  })
  
C. val dfOut = df.rdd
  .map(r => (
       r.getString(0),
       r.getValuesMap[Double](r.schema.fieldNames.filter(_!="e_name"))
     ))
  .map{case (n,m) => (n,m.maxBy(_._2)._1)}
  .toDF("e_name","max_RPA")
  
 D. val maxRPA = maxval($"RPA_1", $"RPA_2", $"RPA_3", $"RPA_4")
 df.withColumn("max_RPA",maxRPA).drop($"RPA_1", $"RPA_2", $"RPA_3", $"RPA_4").show
 
 
 Ans. A,B,C
 
 Explanation: D would give issue on drop syntax; need to drop each RPA


 41. Consider a dataframe df as

id  co   value
1   US    50
1   UK    100
1   Ind   125
2   US    75
2   UK    150
2   Ind   175

we need to convert it and expected df is

id  US  UK   Ind
1   50  100  125
2   75  150  175

choose the correct ways to achieve it from following:

A. val countryValue = udf{(countryToCheck: String, countryInRow: String, value: Long) =>
  if(countryToCheck == countryInRow) value else 0
}
val countryFuncs = countries.map{country => (dataFrame: DataFrame) => dataFrame.withColumn(country, countryValue(lit(country), df("co"), df("value"))) }
val dfWithCountries = Function.chain(countryFuncs)(df).drop("co").drop("value")
dfWithCountries.groupBy("id").sum(countries: _*).show

B. val countries = List("US", "UK", "Ind")
val numCountries = countries.length - 1 

var query = "select *, "
for (i <- 0 to numCountries-1) {
  query += """case when co = """" + countries(i) + """" then value else 0 end as """ + countries(i) + ", "
}
query += """case when co = """" + countries.last + """" then value else 0 end as """ + countries.last + " from myTable"

myDataFrame.registerTempTable("myTable")
val myDF1 = sqlContext.sql(query)

C. spark.sql("select * from countries ").groupBy($"id").pivot("co").agg(min($"value")).show()

D. df.groupBy(col("id"))
        .pivot("co")
        .agg(avg(col("value")));
		
E. df.groupBy("item_id").pivot("week_id").agg(first("sale_amount")).show
		
Ans.: A,B,E

Explanation: C,D would result incorrect value after pivot


42. How we can optimize shuffle operation spill in Apache Spark 1.6+ application?
Choose from following:

A. By increasing shuffle buffer - spark.storage.memoryFraction 

B. Doing repartition() so that you have smaller partitions from input

C. By increasing spark.executor.memory

D. By increasing SPARK_WORKER_CORES

E. By increasing SPARK_DRIVER_MEMORY

Ans.: B,C,D

Explanation: A was deprecated in spark1.5 and is no longer used and E doesn't help in shuffle spill


43. Given a dataframe df as
id |       date
---------------
 1 | 2018-09-01
 2 | 2018-09-01
 1 | 2018-09-03
 1 | 2018-09-04
 2 | 2018-09-04
 
create a running counter, grouped by the same id and sorted by date in that group and expected result is

id |       date |  counter
--------------------------
 1 | 2018-09-01 |        1
 1 | 2018-09-03 |        2
 1 | 2018-09-04 |        3
 2 | 2018-09-01 |        1
 2 | 2018-09-04 |        2
 
 Choose correct answers from following
 
A. val w = Window.partitionBy("id").orderBy("date")
val resultDF = df.select( df("id"), rowNumber().over(w) )

B. val df = sqlContext.sql("select 1, '2018-09-01'"
    ).unionAll(sqlContext.sql("select 2, '2018-09-01'")
    ).unionAll(sqlContext.sql("select 1, '2018-09-03'")
    ).unionAll(sqlContext.sql("select 1, '2018-09-04'")
    ).unionAll(sqlContext.sql("select 2, '2018-09-04'"))

	df.rdd.groupBy(r => r(0)).map(g => g._2.toList.sortBy(row => row(1).toString).zipWithIndex).collect()
	
C.  val w = Window.partitionBy($"id").orderBy($"date")
    df.select($"id", $"date", rowNumber.over(w).alias("counter")).show
	
D. val dfDuplicate = df.selecExpr("id as idDup", "date as dateDup")
val dfWithCounter = df.join(dfDuplicate,$"id"===$"idDup")
                      .where($"date"<=$"dateDup")
                      .groupBy($"id", $"date")
                      .agg($"id", $"date", count($"idDup").as("counter"))
                      .select($"id",$"date",$"counter")
					  
E. df.select("id", "date", 
          rank().over(Window.partitionBy("id").orderBy("date")).alias("counter")).show()
		  
Ans.: A,B,C,D

Explanation: E would give RANK to each row and won't act as counter


44. Consider a PySpark DataFrame containing 

col_1	col_2	col_3
1		2		3
2		1		2
3		4		5

Convert to output dataframe by taking max of each column, output df -
col_4
3
2
5

Select correct answers from following:

A. df.select(greatest("a", "b", "c"))

B. def row_max_with_name(*cols):
    cols_ = [struct(col(c).alias("value"), lit(c).alias("col")) for c in cols]
    return greatest(*cols_).alias("greatest({0})".format(",".join(cols)))

 maxs = df.select(row_max_with_name("a", "b", "c").alias("col_4"))
 
 C. df.select( lit('col_1').alias('cn1'), min(df.col_1).alias('col_1'),
           lit('col_2').alias('cn2'), min(df.col_2).alias('col_2'),
           lit('col_3').alias('cn3'), min(df.col_3).alias('col_3')
          )\
         .rdd.flatMap(lambda r: [ (r.cn1, r.col_1), (r.cn2, r.col_2), (r.cn3, r.col_3)])\
         .toDF(['Columnn', 'col_4']).select("col_4").show()
		 
D. ((_, c), ) = (maxs
    .groupBy(col("maxs")["col"].alias("col"))
    .count()
    .agg(max(struct(col("count"), col("col"))))
    .first())

df.select(c)

E. def row_max(*cols):
    return reduce(
        lambda x, y: when(x > y, x).otherwise(y),
        [col(c) if isinstance(c, str) else c for c in cols]
    )
	df.select(row_max("a", "b", "c").alias("col_4")))
	
Ans. A,B,D,E

Explanation: C would result in max of column


45. How to change case of all row values in pySpark dataframe to lower case or upper case?
Choose all correct answers from following:

A. self.df_db1 =self.df_db1.toDF(*[c.lower() for c in self.df_db1.columns])

B. 
val fields = df.schema.fields
val stringFields = df.schema.fields.filter(f => f.dataType == StringType)
val nonStringFields = df.schema.fields.filter(f => f.dataType != StringType).map(f => f.name).map(f => col(f))

val stringFieldsTransformed = stringFields .map (f => f.name).map(f => upper(col(f)).as(f))
val df = sourceDF.select(stringFieldsTransformed ++ nonStringFields: _*)

C.
 fields = sourceDF.schema.fields
stringFields = filter(lambda f: isinstance(f.dataType, StringType), fields)
nonStringFields = map(lambda f: col(f.name), filter(lambda f: not isinstance(f.dataType, StringType), fields))
stringFieldsTransformed = map(lambda f: upper(col(f.name)), stringFields) 
allFields = [*stringFieldsTransformed, *nonStringFields]
df = sourceDF.select(allFields)

D. for col in df.columns:
    df = sourceDF.withColumn(col, F.lower(F.col(col)))
	
E. expression = [ psf.lower(psf.col(x)).alias(x) for x in sourceDF.columns ]

Ans. C,D,E

Explanation: B is also correct but it will convert numerical values into strings and A is used only for headers




